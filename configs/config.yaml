model:
  name: "Model Name"
  path: "/path/to/original/model"

pruning:
  method: "magnitude"
  selected_layers: "2,4,6,8,10"

optimization:
  algorithm: "genetic"
  num_generations: 5
  population_size: 10

training:
  num_train_epochs: 1
  learning_rate: 2e-5
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  max_seq_length: 512
  output_dir: "/path/to/output"
  save_steps: 500

evaluation:
  model_name: "llama-7b-pruned"
  tasks: ["hellaswag", "piqa", "arc"]
  device: "cuda"
  batch_size: 16
